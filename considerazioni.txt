CONSIDERAZIONI


OPTUNA
utilizzo di optuna per trovare gli hyperparametri ottimi. Questa ottimizzazione va fatta dopo aver raccolto tutti i dati per il training (quindi sul dataset finale) oppure ad ogni step prima del training in modo poi da passare i valori ottimi.

PYTROCH LIGHTNING
semplica e automatizza la procedura di training e validazione con possibilità di visualizzare i risultati su tensorboard.
possibile scale del batch size e del learning rate tramite il tuner 

 INTEGRAZIONE OPTUNA PYTORCH LIGTHNING
 ho integrato i due in modo che ci sia, sia una ricerca dei parametri ottimi, sia un training loop automatico.
 
 In questo modo dopo n_trials, vengono restituiti i parametri ottimi di:
 - learning rate
 - batch_size
 - numero di epoche
 
 Una volta ottenuta dovrà essere eseguito il training con i parametri ottimi.
 
 Quindi, si procede con il dagger e pytroch lightning online.
 
 Una volta concluse le iterazioni, si chiama optuna e si cercano i parametri ottimi per l'ultimo dataset ottenuto con la simulazione.
 
 Si fa nuovamente il training dell'ultima parte e si salva il modello.

 
 OPTUNA esegue il pruned in maniera automatica quando ritiene che il trial che sta eseguendo non è promettente (è un early stopping automatico)
 
 
 
 
 
 ## DAGGER ##
 Training eseguito con macchina a velocità costante sempre.
 Training eseguito con macchina a velocità variabile in curva.
 
 
 
 
 
 
 
 
 
 
 
 
 WORKFLOW
 
 yOLOV8: input(image zed depth image), output(coordinate x,y del centro cono bounding box)
 
 ottenuti i centri, si mappano i punti in birdEye
 
 interpolazione lineare per aumentare il numero di punti del polinomio ed avere una linea "più continua"
 
 viene definito un polinomio con i punti dell'interpolazione precedente per i coni di sinistra e per quelli di destra
 
 si riempiono queste due andando a formare un poligono bianco che rappresenta la strada
 
 
		

 
 
 
 
 
